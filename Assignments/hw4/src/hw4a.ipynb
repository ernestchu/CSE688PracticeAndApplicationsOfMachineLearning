{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE688: Practical and Application of Machine Learning - Spring 2021\n",
    "## Assignment 4a\n",
    "### Authors\n",
    "\n",
    "- B073040018 朱劭璿\n",
    "- B072010029 陳居廷"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 14312\n",
      "Number of validation samples: 35000\n",
      "Number of testing samples: 35000\n",
      "\n",
      "Label: 0.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image \n",
    "\n",
    "def anomaly_detect_split(images, labels, normals=[1, 3, 5, 7]):\n",
    "    '''\n",
    "    normal   = a: 50%, b: 50%\n",
    "    abnormal = c: 50%, d: 50%\n",
    "    training   = a\n",
    "    validation = b + c\n",
    "    testing    = b + d\n",
    "    '''\n",
    "    \n",
    "    normal_mask     = np.isin(labels, normals)\n",
    "    normal_labels   = labels[normal_mask]\n",
    "    normal_images   = images[normal_mask]\n",
    "    abnormal_labels = labels[np.invert(normal_mask)]\n",
    "    abnormal_images = images[np.invert(normal_mask)]\n",
    "    \n",
    "    normal_shuffler = np.random.permutation(len(normal_labels))\n",
    "    train_splitter  = normal_shuffler[:int(len(normal_labels)/2)]\n",
    "    train_labels    = normal_labels[train_splitter]\n",
    "    train_labels    = (train_labels - 1) / 2 # 1, 3, 5, 7 => 0, 1, 2, 3 | For sparse categorical cross entropy\n",
    "    train_images    = normal_images[train_splitter]\n",
    "    \n",
    "    test_normal_splitter = normal_shuffler[int(len(normal_labels)/2):]\n",
    "    val_labels           = np.zeros(len(test_normal_splitter))\n",
    "    val_images           = normal_images[test_normal_splitter]\n",
    "    test_labels          = np.zeros(len(test_normal_splitter))\n",
    "    test_images          = normal_images[test_normal_splitter]\n",
    "    \n",
    "    abnormal_shuffler     = np.random.permutation(len(abnormal_labels))\n",
    "    abnormal_val_splitter = abnormal_shuffler[:int(len(abnormal_labels)/2)]\n",
    "    val_labels            = np.concatenate((val_labels, np.ones(len(abnormal_val_splitter))))\n",
    "    val_images            = np.concatenate((val_images, abnormal_images[abnormal_val_splitter]))\n",
    "    abnormal_test_splitter = abnormal_shuffler[int(len(abnormal_labels)/2):]\n",
    "    test_labels            = np.concatenate((test_labels, np.ones(len(abnormal_test_splitter))))\n",
    "    test_images            = np.concatenate((test_images, abnormal_images[abnormal_test_splitter]))\n",
    "    \n",
    "    # Label preprocessing\n",
    "    \n",
    "    \n",
    "    return (train_images, train_labels, val_images, val_labels, test_images, test_labels)\n",
    "    \n",
    "    \n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "images = np.concatenate((train_images, test_images))\n",
    "labels = np.concatenate((train_labels, test_labels))\n",
    "# Normalize pixel values to be between 0 and 1\n",
    "images = images / 255.0\n",
    "\n",
    "(\n",
    "    train_images, \n",
    "    train_labels, \n",
    "    val_images, \n",
    "    val_labels, \n",
    "    test_images, \n",
    "    test_labels\n",
    ") = anomaly_detect_split(images, labels)\n",
    "\n",
    "print(f'Number of training samples: {train_labels.shape[0]}')\n",
    "print(f'Number of validation samples: {val_labels.shape[0]}')\n",
    "print(f'Number of testing samples: {test_labels.shape[0]}')\n",
    "\n",
    "ds_train = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).cache().shuffle(train_labels.shape[0]).batch(128).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "ds_val = tf.data.Dataset.from_tensor_slices((val_images, val_labels)).cache().batch(128).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "ds_test = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).cache().batch(128).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "print()\n",
    "plt.imshow(train_images[0], cmap='gray')\n",
    "print(f'Label: {train_labels[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape (Reshape)            (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 7, 7, 128)         73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 3, 3, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 3, 3, 256)         295168    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 1, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 390,410\n",
      "Trainable params: 390,410\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classifier = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Reshape((28, 28, 1), input_shape=(28, 28)),\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(10)\n",
    "])\n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-5\n",
    "ATH = 0.8 # anomaly confidence threshold\n",
    "best_classifier = tf.keras.models.clone_model(classifier)\n",
    "best_acc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1\u001b[32m Train \u001b[0m Loss:  2.118, Top-1:  0.266\tAnomaly detection accuracy:\u001b[31m  0.59109\u001b[0m\n",
      "Epoch  2\u001b[32m Train \u001b[0m Loss:  1.607, Top-1:  0.418\tAnomaly detection accuracy:\u001b[31m  0.59109\u001b[0m\n",
      "Epoch  3\u001b[32m Train \u001b[0m Loss:  1.166, Top-1:  0.796\tAnomaly detection accuracy:\u001b[31m  0.59109\u001b[0m\n",
      "Epoch  4\u001b[32m Train \u001b[0m Loss:  0.882, Top-1:  0.852\tAnomaly detection accuracy:\u001b[31m  0.59111\u001b[0m\n",
      "Epoch  5\u001b[32m Train \u001b[0m Loss:  0.658, Top-1:  0.866\tAnomaly detection accuracy:\u001b[31m  0.64006\u001b[0m\n",
      "Epoch  6\u001b[32m Train \u001b[0m Loss:  0.499, Top-1:  0.885\tAnomaly detection accuracy:\u001b[31m  0.71403\u001b[0m\n",
      "Epoch  7\u001b[32m Train \u001b[0m Loss:  0.393, Top-1:  0.903\tAnomaly detection accuracy:\u001b[31m  0.72940\u001b[0m\n",
      "Epoch  8\u001b[32m Train \u001b[0m Loss:  0.321, Top-1:  0.918\tAnomaly detection accuracy:\u001b[31m  0.74206\u001b[0m\n",
      "Epoch  9\u001b[32m Train \u001b[0m Loss:  0.271, Top-1:  0.930\tAnomaly detection accuracy:\u001b[31m  0.73349\u001b[0m\n",
      "Epoch  10\u001b[32m Train \u001b[0m Loss:  0.234, Top-1:  0.938\tAnomaly detection accuracy:\u001b[31m  0.72354\u001b[0m\n",
      "Epoch  11\u001b[32m Train \u001b[0m Loss:  0.206, Top-1:  0.945\tAnomaly detection accuracy:\u001b[31m  0.70757\u001b[0m\n",
      "Epoch  12\u001b[32m Train \u001b[0m Loss:  0.183, Top-1:  0.951\tAnomaly detection accuracy:\u001b[31m  0.69694\u001b[0m\n",
      "Epoch  13\u001b[32m Train \u001b[0m Loss:  0.165, Top-1:  0.956\tAnomaly detection accuracy:\u001b[31m  0.69189\u001b[0m\n",
      "Epoch  14\u001b[32m Train \u001b[0m Loss:  0.151, Top-1:  0.960\tAnomaly detection accuracy:\u001b[31m  0.67629\u001b[0m\n",
      "Epoch  15\u001b[32m Train \u001b[0m Loss:  0.138, Top-1:  0.963\tAnomaly detection accuracy:\u001b[31m  0.67943\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "class customCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        global best_classifier, best_acc\n",
    "        num_correct = 0\n",
    "        num_total = 0\n",
    "        for image, label in ds_val:\n",
    "            confidence = tf.math.reduce_max(tf.nn.softmax(classifier(image)), 1).numpy()\n",
    "            num_correct += ((confidence < ATH) == label.numpy()).sum()\n",
    "            num_total += label.shape[0]\n",
    "            \n",
    "        \n",
    "        print(f\"Epoch {epoch+1: 2d}\", end='')\n",
    "        print(\n",
    "            f\"\\x1b[32m Train \\x1b[0m \"\n",
    "            f\"Loss: {logs['loss']: .3f}, \"\n",
    "            f\"Top-1: {logs['sparse_categorical_accuracy']: .3f}\",\n",
    "            end = '\\t'\n",
    "        )\n",
    "        \n",
    "        acc = num_correct/num_total\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_classifier.set_weights(classifier.get_weights())\n",
    "        print(f'Anomaly detection accuracy:\\x1b[31m {acc: .5f}\\x1b[0m')\n",
    "\n",
    "classifier.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=LR),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['sparse_categorical_accuracy']\n",
    ")\n",
    "history = classifier.fit(\n",
    "    ds_train, \n",
    "    epochs=15,\n",
    "    verbose=0,\n",
    "    callbacks=[customCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly detection accuracy:  0.74031\n"
     ]
    }
   ],
   "source": [
    "num_correct = 0\n",
    "num_total = 0\n",
    "for image, label in ds_test:\n",
    "    confidence = tf.math.reduce_max(tf.nn.softmax(best_classifier(image)), 1).numpy()\n",
    "    num_correct += ((confidence < ATH) == label.numpy()).sum()\n",
    "    num_total += label.shape[0]\n",
    "print(f'Anomaly detection accuracy: {num_correct/num_total: .5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
