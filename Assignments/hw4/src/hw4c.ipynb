{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE688: Practical and Application of Machine Learning - Spring 2021\n",
    "## Assignment 4c\n",
    "### Authors\n",
    "\n",
    "- B073040018 朱劭璿\n",
    "- B072010029 陳居廷"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Please design a denoising DAE only for digits 1, 3, 5, 7 in MNIST. Then use the trained DAE to detect anomaly data that is not in the set of 1, 3, 5, 7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 14312\n",
      "Number of validation samples: 35000\n",
      "Number of testing samples: 35000\n",
      "\n",
      "Label: 1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "from utils import anomaly_detect_split, AnomalyValidation\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "images = np.concatenate((train_images, test_images))\n",
    "labels = np.concatenate((train_labels, test_labels))\n",
    "# Normalize pixel values to be between 0 and 1\n",
    "images = images / 255.0\n",
    "\n",
    "(\n",
    "    train_images, \n",
    "    train_labels, \n",
    "    val_images, \n",
    "    val_labels, \n",
    "    test_images, \n",
    "    test_labels\n",
    ") = anomaly_detect_split(images, labels)\n",
    "\n",
    "print(f'Number of training samples: {train_labels.shape[0]}')\n",
    "print(f'Number of validation samples: {val_labels.shape[0]}')\n",
    "print(f'Number of testing samples: {test_labels.shape[0]}')\n",
    "\n",
    "# Add noise to training data\n",
    "noise_train_images = train_images + np.random.normal(0, .15, train_images.shape)\n",
    "noise_train_images[noise_train_images < 0] = 0.\n",
    "\n",
    "ds_train = tf.data.Dataset.from_tensor_slices((noise_train_images, train_images)).cache().shuffle(train_labels.shape[0]).batch(128).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "ds_val = tf.data.Dataset.from_tensor_slices((val_images, val_images)).cache().batch(128).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "ds_test = tf.data.Dataset.from_tensor_slices((test_images, test_images)).cache().batch(128).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "print()\n",
    "plt.imshow(noise_train_images[0], cmap='gray')\n",
    "print(f'Label: {train_labels[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape (Reshape)            (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 14, 14, 4)         40        \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 7, 7, 8)           296       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 4, 4, 16)          1168      \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 2, 2, 32)          4640      \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 1, 1, 64)          18496     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 2, 2, 32)          18464     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 4, 4, 16)          4624      \n",
      "_________________________________________________________________\n",
      "zero_padding2d (ZeroPadding2 (None, 7, 7, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 14, 14, 4)         580       \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 28, 28, 1)         37        \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 28, 28)            0         \n",
      "=================================================================\n",
      "Total params: 48,345\n",
      "Trainable params: 48,345\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "DAE = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Reshape((28, 28, 1), input_shape=(28, 28)),\n",
    "    tf.keras.layers.Conv2D(4, (3, 3), strides=2, activation='relu', padding='same'),\n",
    "    tf.keras.layers.Conv2D(8, (3, 3), strides=2, activation='relu', padding='same'),\n",
    "    tf.keras.layers.Conv2D(16, (3, 3), strides=2, activation='relu', padding='same'),\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), strides=2, activation='relu', padding='same'),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), strides=2, activation='relu', padding='same'),\n",
    "    tf.keras.layers.Conv2DTranspose(32, (3, 3), strides=2, activation='relu', padding='same'),\n",
    "    tf.keras.layers.Conv2DTranspose(16, (3, 3), strides=2, activation='relu', padding='same'),\n",
    "    tf.keras.layers.ZeroPadding2D(((1, 2), (1, 2))),\n",
    "    tf.keras.layers.Conv2DTranspose(4, (3, 3), strides=2, activation='relu', padding='same'),\n",
    "    tf.keras.layers.Conv2DTranspose(1, (3, 3), strides=2, activation='relu', padding='same'),\n",
    "    tf.keras.layers.Reshape((28, 28))\n",
    "])\n",
    "DAE.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-3\n",
    "ATH = 0.04 # anomaly confidence threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1\u001b[32m Train \u001b[0m MSE:  0.073, \tAnomaly detection accuracy:\u001b[31m  0.07317\u001b[0m\n",
      "Epoch 11\u001b[32m Train \u001b[0m MSE:  0.020, \tAnomaly detection accuracy:\u001b[31m  0.84994\u001b[0m\n",
      "Epoch 21\u001b[32m Train \u001b[0m MSE:  0.017, \tAnomaly detection accuracy:\u001b[31m  0.92697\u001b[0m\n",
      "Epoch 31\u001b[32m Train \u001b[0m MSE:  0.016, \tAnomaly detection accuracy:\u001b[31m  0.96069\u001b[0m\n",
      "Epoch 41\u001b[32m Train \u001b[0m MSE:  0.015, \tAnomaly detection accuracy:\u001b[31m  0.97163\u001b[0m\n",
      "Epoch 51\u001b[32m Train \u001b[0m MSE:  0.015, \tAnomaly detection accuracy:\u001b[31m  0.97829\u001b[0m\n",
      "Epoch 61\u001b[32m Train \u001b[0m MSE:  0.014, \tAnomaly detection accuracy:\u001b[31m  0.98243\u001b[0m\n",
      "Epoch 71\u001b[32m Train \u001b[0m MSE:  0.014, \tAnomaly detection accuracy:\u001b[31m  0.98597\u001b[0m\n",
      "Epoch 81\u001b[32m Train \u001b[0m MSE:  0.014, \tAnomaly detection accuracy:\u001b[31m  0.98886\u001b[0m\n",
      "Epoch 91\u001b[32m Train \u001b[0m MSE:  0.014, \tAnomaly detection accuracy:\u001b[31m  0.99066\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "DAE.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=LR),\n",
    "    loss='mse',\n",
    ")\n",
    "history = DAE.fit(\n",
    "    ds_train, \n",
    "    epochs=100,\n",
    "    verbose=0,\n",
    "    callbacks=[AnomalyValidation(ATH, ds_val, 10)]\n",
    ")\n",
    "# AnomalyValidation callback: Perform anomaly detect on validation dataset\n",
    "#                             Save the best weights and set the weight in training end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly detection accuracy:  0.99183\n"
     ]
    }
   ],
   "source": [
    "num_correct = 0\n",
    "num_total = 0\n",
    "for image, label in ds_test:\n",
    "    num_correct += (tf.keras.losses.MSE(DAE(image), image).numpy().mean(axis=1) < ATH).sum()\n",
    "    num_total += label.shape[0]\n",
    "print(f'Anomaly detection accuracy: {num_correct/num_total: .5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
