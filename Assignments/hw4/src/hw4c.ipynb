{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE688: Practical and Application of Machine Learning - Spring 2021\n",
    "## Assignment 4c\n",
    "### Authors\n",
    "\n",
    "- B073040018 朱劭璿\n",
    "- B072010029 陳居廷"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Please design a denoising DAE only for digits 1, 3, 5, 7 in MNIST. Then use the trained DAE to detect anomaly data that is not in the set of 1, 3, 5, 7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 14312\n",
      "Number of validation samples: 35000\n",
      "Number of testing samples: 35000\n",
      "\n",
      "Label: 0.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "from utils import anomaly_detect_split, AnomalyValidation\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "images = np.concatenate((train_images, test_images))\n",
    "labels = np.concatenate((train_labels, test_labels))\n",
    "# Normalize pixel values to be between 0 and 1\n",
    "images = images / 255.0\n",
    "\n",
    "(\n",
    "    train_images, \n",
    "    train_labels, \n",
    "    val_images, \n",
    "    val_labels, \n",
    "    test_images, \n",
    "    test_labels\n",
    ") = anomaly_detect_split(images, labels)\n",
    "\n",
    "print(f'Number of training samples: {train_labels.shape[0]}')\n",
    "print(f'Number of validation samples: {val_labels.shape[0]}')\n",
    "print(f'Number of testing samples: {test_labels.shape[0]}')\n",
    "\n",
    "# Add noise to training data\n",
    "noise_train_images = train_images + np.random.normal(0, .15, train_images.shape)\n",
    "noise_train_images[noise_train_images < 0] = 0.\n",
    "\n",
    "ds_train = tf.data.Dataset.from_tensor_slices((noise_train_images, train_images)).cache().shuffle(train_labels.shape[0]).batch(128).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "ds_val = tf.data.Dataset.from_tensor_slices((val_images, val_labels)).cache().batch(128).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "ds_test = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).cache().batch(128).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "print()\n",
    "plt.imshow(noise_train_images[0], cmap='gray')\n",
    "print(f'Label: {train_labels[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape (Reshape)            (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 14, 14, 4)         40        \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 7, 7, 8)           296       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 4, 4, 16)          1168      \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 2, 2, 32)          4640      \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 1, 1, 64)          18496     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 2, 2, 32)          18464     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 4, 4, 16)          4624      \n",
      "_________________________________________________________________\n",
      "zero_padding2d (ZeroPadding2 (None, 7, 7, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 14, 14, 4)         580       \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 28, 28, 1)         37        \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 28, 28)            0         \n",
      "=================================================================\n",
      "Total params: 48,345\n",
      "Trainable params: 48,345\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "DAE = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Reshape((28, 28, 1), input_shape=(28, 28)),\n",
    "    tf.keras.layers.Conv2D(4, (3, 3), strides=2, activation='relu', padding='same'),\n",
    "    tf.keras.layers.Conv2D(8, (3, 3), strides=2, activation='relu', padding='same'),\n",
    "    tf.keras.layers.Conv2D(16, (3, 3), strides=2, activation='relu', padding='same'),\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), strides=2, activation='relu', padding='same'),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), strides=2, activation='relu', padding='same'),\n",
    "    tf.keras.layers.Conv2DTranspose(32, (3, 3), strides=2, activation='relu', padding='same'),\n",
    "    tf.keras.layers.Conv2DTranspose(16, (3, 3), strides=2, activation='relu', padding='same'),\n",
    "    tf.keras.layers.ZeroPadding2D(((1, 2), (1, 2))),\n",
    "    tf.keras.layers.Conv2DTranspose(4, (3, 3), strides=2, activation='relu', padding='same'),\n",
    "    tf.keras.layers.Conv2DTranspose(1, (3, 3), strides=2, activation='relu', padding='same'),\n",
    "    tf.keras.layers.Reshape((28, 28))\n",
    "])\n",
    "DAE.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-3\n",
    "ATH = 0.04 # anomaly confidence threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1\u001b[32m Train \u001b[0m MSE:  0.067816, \tAnomaly detection accuracy:\u001b[31m  0.66394\u001b[0m\n",
      "Epoch  2\u001b[32m Train \u001b[0m MSE:  0.046949, \tAnomaly detection accuracy:\u001b[31m  0.75743\u001b[0m\n",
      "Epoch  3\u001b[32m Train \u001b[0m MSE:  0.035293, \tAnomaly detection accuracy:\u001b[31m  0.78874\u001b[0m\n",
      "Epoch  4\u001b[32m Train \u001b[0m MSE:  0.030108, \tAnomaly detection accuracy:\u001b[31m  0.74680\u001b[0m\n",
      "Epoch  5\u001b[32m Train \u001b[0m MSE:  0.026526, \tAnomaly detection accuracy:\u001b[31m  0.69457\u001b[0m\n",
      "Epoch  6\u001b[32m Train \u001b[0m MSE:  0.024448, \tAnomaly detection accuracy:\u001b[31m  0.64746\u001b[0m\n",
      "Epoch  7\u001b[32m Train \u001b[0m MSE:  0.023055, \tAnomaly detection accuracy:\u001b[31m  0.62246\u001b[0m\n",
      "Epoch  8\u001b[32m Train \u001b[0m MSE:  0.022051, \tAnomaly detection accuracy:\u001b[31m  0.59754\u001b[0m\n",
      "Epoch  9\u001b[32m Train \u001b[0m MSE:  0.021261, \tAnomaly detection accuracy:\u001b[31m  0.57363\u001b[0m\n",
      "Epoch 10\u001b[32m Train \u001b[0m MSE:  0.020619, \tAnomaly detection accuracy:\u001b[31m  0.55057\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "DAE.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=LR),\n",
    "    loss='mse',\n",
    ")\n",
    "history = DAE.fit(\n",
    "    ds_train, \n",
    "    epochs=10,\n",
    "    verbose=0,\n",
    "    callbacks=[AnomalyValidation(ATH, ds_val)]\n",
    ")\n",
    "# AnomalyValidation callback: Perform anomaly detect on validation dataset\n",
    "#                             Save the best weights and set the weight in training end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly detection accuracy:  0.78849\n"
     ]
    }
   ],
   "source": [
    "num_correct = 0\n",
    "num_total = 0\n",
    "for image, label in ds_test:\n",
    "    num_correct += ((tf.keras.losses.MSE(DAE(image), image).numpy().mean(axis=1) > ATH) == label.numpy()).sum()\n",
    "    num_total += label.shape[0]\n",
    "print(f'Anomaly detection accuracy: {num_correct/num_total: .5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
