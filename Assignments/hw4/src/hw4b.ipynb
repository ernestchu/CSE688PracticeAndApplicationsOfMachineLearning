{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE688: Practical and Application of Machine Learning - Spring 2021\n",
    "## Assignment 4b\n",
    "### Authors\n",
    "\n",
    "- B073040018 朱劭璿\n",
    "- B072010029 陳居廷"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Please design an autoencoder only for digits 1, 3, 5, 7 in MNIST. Then use the trained autoencoder to detect anomaly data that is not in the set of 1, 3, 5, 7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 14312\n",
      "Number of validation samples: 35000\n",
      "Number of testing samples: 35000\n",
      "\n",
      "Label: 3.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "from utils import anomaly_detect_split, AnomalyValidation\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "images = np.concatenate((train_images, test_images))\n",
    "labels = np.concatenate((train_labels, test_labels))\n",
    "# Normalize pixel values to be between 0 and 1\n",
    "images = images / 255.0\n",
    "\n",
    "(\n",
    "    train_images, \n",
    "    train_labels, \n",
    "    val_images, \n",
    "    val_labels, \n",
    "    test_images, \n",
    "    test_labels\n",
    ") = anomaly_detect_split(images, labels)\n",
    "\n",
    "print(f'Number of training samples: {train_labels.shape[0]}')\n",
    "print(f'Number of validation samples: {val_labels.shape[0]}')\n",
    "print(f'Number of testing samples: {test_labels.shape[0]}')\n",
    "\n",
    "ds_train = tf.data.Dataset.from_tensor_slices((train_images, train_images)).cache().shuffle(train_labels.shape[0]).batch(128).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "ds_val = tf.data.Dataset.from_tensor_slices((val_images, val_labels)).cache().batch(128).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "ds_test = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).cache().batch(128).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "print()\n",
    "plt.imshow(train_images[0], cmap='gray')\n",
    "print(f'Label: {train_labels[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape (Reshape)            (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 14, 14, 4)         40        \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 7, 7, 8)           296       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 4, 4, 16)          1168      \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 2, 2, 32)          4640      \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 1, 1, 64)          18496     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 2, 2, 32)          18464     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 4, 4, 16)          4624      \n",
      "_________________________________________________________________\n",
      "zero_padding2d (ZeroPadding2 (None, 7, 7, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 14, 14, 4)         580       \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 28, 28, 1)         37        \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 28, 28)            0         \n",
      "=================================================================\n",
      "Total params: 48,345\n",
      "Trainable params: 48,345\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Reshape((28, 28, 1), input_shape=(28, 28)),\n",
    "    tf.keras.layers.Conv2D(4, (3, 3), strides=2, activation='relu', padding='same'),\n",
    "    tf.keras.layers.Conv2D(8, (3, 3), strides=2, activation='relu', padding='same'),\n",
    "    tf.keras.layers.Conv2D(16, (3, 3), strides=2, activation='relu', padding='same'),\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), strides=2, activation='relu', padding='same'),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), strides=2, activation='relu', padding='same'),\n",
    "    tf.keras.layers.Conv2DTranspose(32, (3, 3), strides=2, activation='relu', padding='same'),\n",
    "    tf.keras.layers.Conv2DTranspose(16, (3, 3), strides=2, activation='relu', padding='same'),\n",
    "    tf.keras.layers.ZeroPadding2D(((1, 2), (1, 2))),\n",
    "    tf.keras.layers.Conv2DTranspose(4, (3, 3), strides=2, activation='relu', padding='same'),\n",
    "    tf.keras.layers.Conv2DTranspose(1, (3, 3), strides=2, activation='relu', padding='same'),\n",
    "    tf.keras.layers.Reshape((28, 28))\n",
    "])\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-3\n",
    "ATH = 0.03 # anomaly confidence threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1\u001b[32m Train \u001b[0m MSE:  0.069909, \tAnomaly detection accuracy:\u001b[31m  0.60134\u001b[0m\n",
      "Epoch  2\u001b[32m Train \u001b[0m MSE:  0.052370, \tAnomaly detection accuracy:\u001b[31m  0.69986\u001b[0m\n",
      "Epoch  3\u001b[32m Train \u001b[0m MSE:  0.037248, \tAnomaly detection accuracy:\u001b[31m  0.75383\u001b[0m\n",
      "Epoch  4\u001b[32m Train \u001b[0m MSE:  0.030431, \tAnomaly detection accuracy:\u001b[31m  0.77694\u001b[0m\n",
      "Epoch  5\u001b[32m Train \u001b[0m MSE:  0.027046, \tAnomaly detection accuracy:\u001b[31m  0.76946\u001b[0m\n",
      "Epoch  6\u001b[32m Train \u001b[0m MSE:  0.025086, \tAnomaly detection accuracy:\u001b[31m  0.75811\u001b[0m\n",
      "Epoch  7\u001b[32m Train \u001b[0m MSE:  0.023631, \tAnomaly detection accuracy:\u001b[31m  0.73989\u001b[0m\n",
      "Epoch  8\u001b[32m Train \u001b[0m MSE:  0.022471, \tAnomaly detection accuracy:\u001b[31m  0.72714\u001b[0m\n",
      "Epoch  9\u001b[32m Train \u001b[0m MSE:  0.021612, \tAnomaly detection accuracy:\u001b[31m  0.71580\u001b[0m\n",
      "Epoch 10\u001b[32m Train \u001b[0m MSE:  0.020911, \tAnomaly detection accuracy:\u001b[31m  0.70371\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "autoencoder.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=LR),\n",
    "    loss='mse',\n",
    ")\n",
    "history = autoencoder.fit(\n",
    "    ds_train, \n",
    "    epochs=10,\n",
    "    verbose=0,\n",
    "    callbacks=[AnomalyValidation(ATH, ds_val)]\n",
    ")\n",
    "# AnomalyValidation callback: Perform anomaly detect on validation dataset\n",
    "#                             Save the best weights and set the weight in training end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly detection accuracy:  0.77594\n"
     ]
    }
   ],
   "source": [
    "num_correct = 0\n",
    "num_total = 0\n",
    "for image, label in ds_test:\n",
    "    num_correct += ((tf.keras.losses.MSE(autoencoder(image), image).numpy().mean(axis=1) > ATH) == label.numpy()).sum()\n",
    "    num_total += label.shape[0]\n",
    "print(f'Anomaly detection accuracy: {num_correct/num_total: .5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
