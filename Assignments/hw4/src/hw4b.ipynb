{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE688: Practical and Application of Machine Learning - Spring 2021\n",
    "## Assignment 4b\n",
    "### Authors\n",
    "\n",
    "- B073040018 朱劭璿\n",
    "- B072010029 陳居廷"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Please design an autoencoder only for digits 1, 3, 5, 7 in MNIST. Then use the trained autoencoder to detect anomaly data that is not in the set of 1, 3, 5, 7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 14312\n",
      "Number of validation samples: 35000\n",
      "Number of testing samples: 35000\n",
      "\n",
      "Label: 1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "from utils import anomaly_detect_split, AnomalyValidation\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "images = np.concatenate((train_images, test_images))\n",
    "labels = np.concatenate((train_labels, test_labels))\n",
    "# Normalize pixel values to be between 0 and 1\n",
    "images = images / 255.0\n",
    "\n",
    "(\n",
    "    train_images, \n",
    "    train_labels, \n",
    "    val_images, \n",
    "    val_labels, \n",
    "    test_images, \n",
    "    test_labels\n",
    ") = anomaly_detect_split(images, labels)\n",
    "\n",
    "print(f'Number of training samples: {train_labels.shape[0]}')\n",
    "print(f'Number of validation samples: {val_labels.shape[0]}')\n",
    "print(f'Number of testing samples: {test_labels.shape[0]}')\n",
    "\n",
    "ds_train = tf.data.Dataset.from_tensor_slices((train_images, train_images)).cache().shuffle(train_labels.shape[0]).batch(128).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "ds_val = tf.data.Dataset.from_tensor_slices((val_images, val_images)).cache().batch(128).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "ds_test = tf.data.Dataset.from_tensor_slices((test_images, test_images)).cache().batch(128).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "print()\n",
    "plt.imshow(train_images[0], cmap='gray')\n",
    "print(f'Label: {train_labels[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape (Reshape)            (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 14, 14, 4)         40        \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 7, 7, 8)           296       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 4, 4, 16)          1168      \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 2, 2, 32)          4640      \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 1, 1, 64)          18496     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 2, 2, 32)          18464     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 4, 4, 16)          4624      \n",
      "_________________________________________________________________\n",
      "zero_padding2d (ZeroPadding2 (None, 7, 7, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 14, 14, 4)         580       \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 28, 28, 1)         37        \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 28, 28)            0         \n",
      "=================================================================\n",
      "Total params: 48,345\n",
      "Trainable params: 48,345\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Reshape((28, 28, 1), input_shape=(28, 28)),\n",
    "    tf.keras.layers.Conv2D(4, (3, 3), strides=2, activation='relu', padding='same'),\n",
    "    tf.keras.layers.Conv2D(8, (3, 3), strides=2, activation='relu', padding='same'),\n",
    "    tf.keras.layers.Conv2D(16, (3, 3), strides=2, activation='relu', padding='same'),\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), strides=2, activation='relu', padding='same'),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), strides=2, activation='relu', padding='same'),\n",
    "    tf.keras.layers.Conv2DTranspose(32, (3, 3), strides=2, activation='relu', padding='same'),\n",
    "    tf.keras.layers.Conv2DTranspose(16, (3, 3), strides=2, activation='relu', padding='same'),\n",
    "    tf.keras.layers.ZeroPadding2D(((1, 2), (1, 2))),\n",
    "    tf.keras.layers.Conv2DTranspose(4, (3, 3), strides=2, activation='relu', padding='same'),\n",
    "    tf.keras.layers.Conv2DTranspose(1, (3, 3), strides=2, activation='relu', padding='same'),\n",
    "    tf.keras.layers.Reshape((28, 28))\n",
    "])\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-3\n",
    "ATH = 0.03 # anomaly confidence threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1\u001b[32m Train \u001b[0m MSE:  0.074, \tAnomaly detection accuracy:\u001b[31m  0.01580\u001b[0m\n",
      "Epoch 11\u001b[32m Train \u001b[0m MSE:  0.019, \tAnomaly detection accuracy:\u001b[31m  0.64603\u001b[0m\n",
      "Epoch 21\u001b[32m Train \u001b[0m MSE:  0.017, \tAnomaly detection accuracy:\u001b[31m  0.77714\u001b[0m\n",
      "Epoch 31\u001b[32m Train \u001b[0m MSE:  0.015, \tAnomaly detection accuracy:\u001b[31m  0.85300\u001b[0m\n",
      "Epoch 41\u001b[32m Train \u001b[0m MSE:  0.015, \tAnomaly detection accuracy:\u001b[31m  0.89169\u001b[0m\n",
      "Epoch 51\u001b[32m Train \u001b[0m MSE:  0.014, \tAnomaly detection accuracy:\u001b[31m  0.91523\u001b[0m\n",
      "Epoch 61\u001b[32m Train \u001b[0m MSE:  0.014, \tAnomaly detection accuracy:\u001b[31m  0.93037\u001b[0m\n",
      "Epoch 71\u001b[32m Train \u001b[0m MSE:  0.014, \tAnomaly detection accuracy:\u001b[31m  0.94171\u001b[0m\n",
      "Epoch 81\u001b[32m Train \u001b[0m MSE:  0.013, \tAnomaly detection accuracy:\u001b[31m  0.94826\u001b[0m\n",
      "Epoch 91\u001b[32m Train \u001b[0m MSE:  0.013, \tAnomaly detection accuracy:\u001b[31m  0.95363\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "autoencoder.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=LR),\n",
    "    loss='mse',\n",
    ")\n",
    "history = autoencoder.fit(\n",
    "    ds_train, \n",
    "    epochs=100,\n",
    "    verbose=0,\n",
    "    callbacks=[AnomalyValidation(ATH, ds_val, 10)]\n",
    ")\n",
    "# AnomalyValidation callback: Perform anomaly detect on validation dataset\n",
    "#                             Save the best weights and set the weight in training end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly detection accuracy:  0.95683\n"
     ]
    }
   ],
   "source": [
    "num_correct = 0\n",
    "num_total = 0\n",
    "for image, label in ds_test:\n",
    "    num_correct += (tf.keras.losses.MSE(autoencoder(image), image).numpy().mean(axis=1) < ATH).sum()\n",
    "    num_total += label.shape[0]\n",
    "print(f'Anomaly detection accuracy: {num_correct/num_total: .5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
